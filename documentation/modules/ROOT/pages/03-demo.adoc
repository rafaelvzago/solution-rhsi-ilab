= See the Solution in Action

== Demonstration

References:

* https://github.com/instructlab[Commands extract from the InstructLab project]
* https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab[Getting started with InstructLab: Generative AI model tuning]
* https://github.com/instructlab/instructlab/blob/main/README.md#-installing-ilab[InstructLab installation guide]
* https://github.com/rafaelvzago/ilab-client[ILAB Frontend chatbot]
* https://developers.redhat.com[Red Hat Developer]

== Concepts and Commands Used in the Demonstration

[NOTE]
====
Please make sure to explain what each Skupper command does the first time you use it, especially for people unfamiliar with Skupper. The following commands should be explained:

- **`skupper init`**: Initializes the Skupper network, setting up the necessary components to enable secure communication between services.
- **`skupper expose`**: Exposes a local service through the Skupper network, allowing it to be accessed from other Skupper-connected sites.
- **`skupper token`**: Generates a connection token that can be used by other sites to link to the Skupper network, ensuring secure communication.
- **`skupper link`**: Establishes a secure link between two Skupper sites using the token created by `skupper token`.
- **`skupper service status`**: Displays the status of services exposed through Skupper, showing what is accessible and how itâ€™s connected within the network.
- **`ilab download`**: Downloads the model to be used by the chatbot.
- **`ilab model serve`**: Starts the server that will be responsible for receiving the user input, sending it to the LLaMA3 model, and sending the response back to the user.
- **`ilab model chat`**: Starts the chatbot, allowing the user to interact with it.

====

[#_demonstration]
== Run the demonstration

[#_before_getting_started]
=== Before getting started

To set up the demonstration, you need to have the following prerequisites:

* Access to an OpenShift cluster with the Skupper and InstructLab installed.
* A server running the InstructLab chat model.
* Access to a terminal to run the commands.
* Access to a web browser to interact with the chatbot.
* oc client installed and configured to access the OpenShift cluster.
* skupper client installed and configured to access the OpenShift cluster.
* Podman installed to run the private Skupper.


[#_getting_started]

[#_ai_model_deployment_with_instructlab]
=== AI Model Deployment with InstructLab

The first step is to deploy the InstructLab chat model in the InstructLab site. The InstructLab chat model will be responsible for receiving the user input and sending it to the LLaMA3 model. The response from the LLaMA3 model will be sent back to the user. This is based on the article: https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab[Getting Started with InstructLab for Generative AI Model Tuning].

[.console-input]
[source,shell script]
----
mkdir instructlab && cd instructlab
python3.11 -m venv venv
source venv/bin/activate
pip install 'instructlab[cuda]' -C cmake.args="-DLLAMA_CUDA=on" -C cmake.args="-DLLAMA_NATIVE=off"
----

[IMPORTANT]
====
This installation method will enable your Nvidia GPU to be used by InstructLab. If you don't have an Nvidia GPU, please check other options in: https://github.com/instructlab/instructlab/blob/main/README.md#-installing-ilab[InstructLab Installation Guide].
====

=== Initialize the InstructLab Configuration

After installing InstructLab, you need to initialize the configuration. This is the step where you create the `config.yaml` file, at `~/instructlab/config.yaml`, with the default configuration. To initialize the configuration, run the following command:

[.console-input]
[source,shell script]
----
ilab config init
----

To enable external access to your model, please modify the `config.yaml` file, located into your `instructlab` directory ie. `~/instructlab/config.yaml`, with the following content:

[source,yaml]
----
chat:
  context: default
  greedy_mode: false
  logs_dir: data/chatlogs
  max_tokens: null
  model: models/granite-7b-lab-Q4_K_M.gguf
  session: null
  vi_mode: false
  visible_overflow: true
general:
  log_level: INFO
generate:
  chunk_word_count: 1000
  model: models/granite-7b-lab-Q4_K_M.gguf
  num_cpus: 10
  num_instructions: 100
  output_dir: generated
  prompt_file: prompt.txt
  seed_file: seed_tasks.json
  taxonomy_base: origin/main
  taxonomy_path: taxonomy
serve:
  gpu_layers: -1
  host_port: 0.0.0.0:8000
  max_ctx_size: 4096
  model_path: models/granite-7b-lab-Q4_K_M.gguf
----

=== Download the model base

Now you need to download the model, this is the step where you download the model to be used by the chatbot. The model is the granite-7b-lab-Q4_K_M.gguf model, which is a large language model trained on the Merlinite dataset. To download the model, run the following command:

[.console-input]
[source,shell script]
----
ilab download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

[#_training_the_model]
== Training the model (Optional)

Now you have 2 options to train the model, you can use the InstructLab to train the model or you can use the pre-trained model. In this case, we will use the pre-trained model. The next steps into this section are optional, we will work with the model alignment and training with InstructLab. If dont want to train the model, please skip this section.

=== Introduction to Model Alignment and Training with InstructLab

The pre-trained model is a large language model trained on the granite dataset. The model is capable of generating human-like responses to user input and can be used to create engaging and interactive applications. In this section, we will explore how to align and train the model using InstructLab, a tool that simplifies the process of training and fine-tuning generative AI models.

In this example, we will add new information into our model, to achieve this, we will work with the taxonomy. The taxonomy is a hierarchical structure that organizes the information used to train the model. By adding new information to the taxonomy, we can improve the model's ability to generate relevant and accurate responses to user input.

[IMPORTANT]
====
* The taxonomy is a key component of the training process, as it provides the model with the knowledge it needs to generate responses to user input. By adding new information to the taxonomy, we can improve the model's performance and create more engaging and interactive applications.
====

[NOTE]
====
* If you want a deep dive into the model alignment and training with InstructLab, please check the article: https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab[Getting Started with InstructLab for Generative AI Model Tuning].
====

=== Adding knowledge to the taxonomy

Let's create a new file under the `taxonomy` directory, located at `~/instructlab/taxonomy/knowledge/trivia/redhat/qna.yaml`. This file will contain information about curiosities from Red Hat, Open Source, and the community. To create the file, run the following command:

[.console-input]
[source,shell script]
----
mkdir -p taxonomy/knowledge/trivia/redhat
touch taxonomy/knowledge/trivia/redhat/qna.yaml
----

If you check the `qna.yaml` file, you will see the following content:

[source,yaml]
----
version: 2
task_description: 'Teach the model about curiosities from Red Hat, Open Source, and the community'
created_by: rafaelvzago
domain: technology
seed_examples:
  - question: What is the origin of the name "Red Hat" in Red Hat Inc.?
    answer: |
      The name "Red Hat" comes from co-founder Marc Ewing's red Cornell University lacrosse hat, which he wore
      while working on early versions of Red Hat Linux.
  - question: When was Red Hat founded and by whom?
    answer: |
      Red Hat was founded in 1993 by Bob Young and Marc Ewing.
  - question: What is the Fedora Project and its relationship with Red Hat?
    answer: |
      The Fedora Project is a community-driven Linux distribution sponsored by Red Hat. It serves as the upstream
      source for innovations that may be included in Red Hat Enterprise Linux.
  - question: How did IBM's acquisition of Red Hat in 2019 impact the open-source community?
    answer: |
      IBM's $34 billion acquisition of Red Hat in 2019 was one of the largest in tech history, reinforcing the
      importance of open-source software in enterprise solutions and expanding Red Hat's global reach.
  - question: What is OpenShift and why is it significant in the open-source community?
    answer: |
      OpenShift is Red Hat's open-source container application platform based on Kubernetes. It enables developers
      to build, deploy, and manage containerized applications, promoting cloud-native development practices.
document:
  repo: https://github.com/rafaelvzago/red-hat-opensource-curiosities.git
  commit: a70c2cd
  patterns:
    - curiosities.md
----

[IMPORTANT]
====
* Left no blank lines between the YAML keys and values, neither at the end of the file.
====

This is the content of the `qna.yaml` file, which contains information about curiosities from Red Hat, Open Source, and the community. The file includes questions and answers related to the origin of the name "Red Hat," the founding of Red Hat, the Fedora Project, IBM's acquisition of Red Hat, and OpenShift. This information will be used to train the model and improve its ability to generate responses to user input.

=== Training the model with the new knowledge

Now that we have added new information to the taxonomy, we can train the model with this knowledge. To train the model, run the following command:

[.console-input]
[source,shell script]
----
ilab taxonomy diff
ilab data generate
ilab model train
ilab model convert
ilab model serve --model-path NEW_MODEL_PATH/MODEL_NAME.gguf
----

Let's break down the commands:

[NOTE]
====
* The `ilab taxonomy diff` command is used to check the differences between the current taxonomy and the new taxonomy file.
* The `ilab data generate` command is used to generate the training data for the model based on the information in the taxonomy.
* The `ilab model train` command is used to train the model with the new knowledge from the taxonomy.
* The `ilab model convert` command is used to convert the trained model to a format that can be used by the chatbot.
* The `ilab model serve --model-path NEW_MODEL_PATH/MODEL_NAME.gguf` command is used to start the server with the new model, allowing the chatbot to interact with it.
====

[IMPORTANT]
====
* The training requires a significant amount of computational resources and time, depending on the size of the model and the complexity of the training data. Make sure you have the necessary resources available before starting the training process.
* The training part could take a long time, depending on the size of the model and the complexity of the training data. Make sure you have the necessary resources available before starting the training process. At my machine, the training took around 20 hours:
* Specs:
** 13th Gen Intel(R) Core(TM) i7-1365U
** 32GB RAM
====

=== Conclusion of the training section

Our goal with this solution is to show how to deploy the InstructLab chat model and interact with it using the ILAB Frontend chatbot. The training section is optional, and you can skip it if you want to use the pre-trained model. The next steps will focus on deploying the InstructLab chat model and interacting with it using the ILAB Frontend chatbot.

=== Start the server

The last step is to start the server. The server will be responsible for receiving the user input, sending it to the LLaMA3 model, and sending the response back to the user. If you have trained the model, you can use the new model path. If you are using the pre-trained model, you can use the default model path. To start the server, run the following command:

[.console-input]
[source,shell script]
----
ilab model serve

# The output should be similar to:
INFO 2024-07-30 18:59:01,199 serve.py:51: serve Using model 'models/granite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-07-30 18:59:01,611 server.py:218: server Starting server process, press CTRL+C to shutdown server...
INFO 2024-07-30 18:59:01,612 server.py:219: server After application startup complete see http://0.0.0.0:8000/docs for API.
----

[#_public_skupper_deployment]
== Public Skupper Deployment

Deploy the public Skupper in Openshift Cluster. The public Skupper will receive the connection from the private Skupper and create a secure connection between the two sites.

=== Creating the project and deploying the public Skupper:

This is the step where you create the project and deploy the public Skupper. The public Skupper will be responsible for receiving the connection from the private Skupper and creating a secure connection between the two sites. Open a new terminal and run the following commands:


[.console-input]
[source,shell script]
----
export SKUPPER_PLATFORM=kubernetes
oc new-project ollama-pilot
skupper init --enable-console --enable-flow-collector --console-user admin --console-password admin
----

[IMPORTANT]
====
* Run this command in a new terminal and keep it open, because the default platform is `kubernetes` and the private terminal is using `podman`.
====

[NOTE]
====
* `SKUPPER_PLATFORM=kubernetes` is used to set the platform to Kubernetes. This is necessary because the public Skupper will be running on a Kubernetes cluster.
* `oc new-project ollama-pilot` is used to create a new project called `ollama-pilot`.
* `skupper init` is used to initialize the Skupper network, setting up the necessary components to enable secure communication between services.
* The `--enable-console` flag is used to enable the Skupper console, which provides a web interface for managing the Skupper network.
* The `--enable-flow-collector` flag is used to enable the flow collector, which collects and displays information about the traffic flowing through the Skupper network.
* The `--console-user admin` flag is used to set the username for the Skupper console to `admin`.
* The `--console-password admin` flag is used to set the password for the Skupper console to `admin`.
====

=== Creating the token to allow the private Skupper to connect to the public Skupper:

This is the step where you create the token to allow the private Skupper to connect to the public Skupper. At the same terminal, run the following command:

[.console-input]
[source,shell script]
----
skupper token create token.yaml
----

[NOTE]
====
* `skupper token create token.yaml` is used to generate a connection token that can be used by other sites to link to the Skupper network, ensuring secure communication.
* The `token.yaml` file will contain the token to connect the two sites.
====

Now, you'll have a `token.yaml` file with the token to connect the two sites.

[#_private_skupper_deployment]
== Private Skupper Deployment

The second step is to deploy the private Skupper in Private Local Environment. The private Skupper will be responsible for creating a secure connection between the two sites, allowing the Ollama Pilot application to send requests to the InstructLab chat model. 

=== Install Skupper

To install skupper on site A, with podman as the platform, open a new terminal to handle all the commands related to the private Skupper.

[.console-input]
[source,shell script]
----
export SKUPPER_PLATFORM=podman
skupper init --ingress none
----

[NOTE]
====
* `SKUPPER_PLATFORM=podman` is used to set the platform to podman. This is necessary because the private Skupper will be running on a podman container.
* `skupper init` is used to initialize the Skupper network, setting up the necessary components to enable secure communication between services.
* The `--ingress none` flag is used to disable the automatic creation of an ingress controller. This is necessary because the public Skupper will be responsible for exposing the service to the internet.
====

=== Exposing the InstructLab Chat Model

To bind the local service running the InstructLab chat model to the Skupper service:

[.console-input]
[source,shell script]
----
skupper expose host host.containers.internal --address instructlab --port 8000
----

[NOTE]
====
* `skupper expose` is used to expose a local service through the Skupper network, allowing it to be accessed from other Skupper-connected sites.
* `host.containers.internal` is used to bind the local service to the Skupper service.
* `--address instructlab` is used to specify the address of the service.
* `--port 8000` is used to specify the port of the service.
====

Check the status of the Skupper service:

[.console-input]
[source,shell script]
----
skupper service status

Services exposed through Skupper:
â•°â”€ instructlab:8000 (tcp)
----

[NOTE]
====
* `skupper service status` is used to display the status of services exposed through Skupper, showing what is accessible and how itâ€™s connected within the network.
====

=== Secure Communication Between the Two Sites with Skupper

Now it's time to establish a secure connection between the two sites using the token created by the public Skupper. Using the token created by the public Skupper, run the following command at the terminal where the private Skupper is running:

[.console-input]
[source,shell script]
----
skupper link create token.yaml --name instructlab
----

[NOTE]
====
* `skupper link create token.yaml --name instructlab` is used to establish a secure link between two Skupper sites using the token created by `skupper token`.
====

Check the status of the Skupper link:

[.console-input]
[source,shell script]
----
skupper link status

Links created from this site:

        Link instructlab is connected

Current links from other sites that are connected:

        There are no connected links
----

[NOTE]
====
* `skupper link status` is used to display the status of the links created by the Skupper network, showing which sites are connected and how they are connected.
====

Check the status on the public Skupper terminal:

[.console-input]
[source,shell script]
----
skupper link status

Links created from this site:

       There are no links configured or connected

Current links from other sites that are connected:

       Incoming link from site b8ad86d5-9680-4fea-9c07-ea7ee394e0bd
----

[NOTE]
====
* `skupper link status` is used to display the status of the links created by the Skupper network, showing which sites are connected and how they are connected.
====

=== Chatbot with Protected Data

The last step is to expose the service in the public Skupper and create the Ollama Pilot application.

* Still on the terminal where the **public** Skupper is running, run the following command to expose the service:

[.console-input]
[source,shell script]
----
skupper service create instructlab 8000
----

[NOTE]
====
* `skupper service create instructlab 8000` is used to create the service in the public Skupper, allowing it to be accessed from the private Skupper.
====

[#_deploying_instructlab_chatbot]
== Deploying the InstructLab Chatbot

Before run the chatbot, let's understand the final part of this solution, the Frontend application.

This application will be deployed in a OpenShift cluster, and will be responsible for sending the user input to the InstructLab chat model and displaying the response to the user. The application will be deployed at the same namespace where the public Skupper is running.


[IMPORTANT]
====
* The frontend application called ILAB Frontend chatbot will use the local service in the public cluster to send the user input to the InstructLab chat model and display the response to the user. See the line 23 of the `ilab-client-deployment.yaml` file.
====

=== Deploy ILAB Frontend chatbot

To deploy the ILAB Frontend chatbot, lets use the following yaml deployment file, in this case the file is located at `~/instructlab/ilab-client-deployment.yaml`:

[.source,yaml]
----
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  name: ilab-client
spec:
  replicas: 1
  selector:
    app: ilab-client
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: ilab-client
    spec:
      containers:
      - name: ilab-client-container
        image: quay.io/rzago/ilab-client:latest
        ports:
        - containerPort: 5000
        env:
        - name: ADDRESS
          value: "http://instructlab:8000" # The address of the InstructLab chat model connected to the private Skupper
  triggers:
  - type: ConfigChange
----

Apply the deployment file:

[.console-input]
[source,shell script]
----
oc apply -f ~/instructlab/ilab-client-deployment.yaml
----

[NOTE]
====
* The `ilab-client-deployment.yaml` file is used to deploy the ILAB Frontend chatbot, which will be responsible for sending the user input to the InstructLab chat model and displaying the response to the user.
* The `ADDRESS` environment variable is used to specify the address of the InstructLab chat model connected to the private Skupper.
* The `oc apply -f ~/instructlab/ilab-client-deployment.yaml` command is used to apply the deployment file and deploy the ILAB Frontend chatbot.
====

=== Creating the service of the ILAB Frontend chatbot deployment

Now, let's create the service for the ILAB Frontend chatbot deployment, the file is located at `~/instructlab/ilab-client-service.yaml`:

[.source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: ilab-client-service
spec:
  selector:
    app: ilab-client
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
----

Apply the service file:

[.console-input]
[source,shell script]
----
oc apply -f ~/instructlab/ilab-client-service.yaml
----

[NOTE]
====
* The `ilab-client-service.yaml` file is used to create the service for the ILAB Frontend chatbot deployment.
* The `oc apply -f ~/instructlab/ilab-client-service.yaml` command is used to apply the service file and create the service for the ILAB Frontend chatbot deployment.
====

=== Exposing the service of the ILAB Frontend chatbot deployment

We are almost there, now let's expose the service of the ILAB Frontend chatbot deployment, the file is located at `~/instructlab/ilab-client-route.yaml`:

[.source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: ilab-client-route
spec:
  to:
    kind: Service
    name: ilab-client-service
  port:
    targetPort: 5000
----

Apply the route file:

[.console-input]
[source,shell script]
----
oc apply -f ~/instructlab/ilab-client-route.yaml
----

[NOTE]
====
* The `ilab-client-route.yaml` file is used to expose the service of the ILAB Frontend chatbot deployment.
* The `oc apply -f ~/instructlab/ilab-client-route.yaml` command is used to apply the route file and expose the service of the ILAB Frontend chatbot deployment.
====

=== Accessing the ILAB Frontend chatbot

Finally, to access the ILAB Frontend chatbot, you can use the following command to get the public URL:

[.console-input]
[source,shell script]
----
oc get route ilab-client-route
----

[NOTE]
====
* The `oc get route ilab-client-route` command is used to get the public URL of the ILAB Frontend chatbot, which will be used to access the chatbot from the Ollama Pilot application.
====

[#_interacting_with_the_chatbot]
== Interacting with the chatbot

To interact with the chatbot, you can access the public URL of the ILAB Frontend chatbot using a web browser. The chatbot will be displayed on the screen, and you can start interacting with it by typing messages in the chat window.

image::chat_bot.png[Chatbot]

[#_considerations]
=== Considerations

* If your machine has an Nvidia GPU, you can use the InstructLab chat model to generate responses to user input. The InstructLab chat model is a large language model trained on the Merlinite dataset and is capable of generating human-like responses to user input. By following the steps outlined in this demonstration, you can deploy the InstructLab chat model in your environment and interact with it using the ILAB Frontend chatbot. This will allow you to experience the power of generative AI models and see how they can be used to create engaging and interactive applications.
* Adjust the model temperature to control the randomness of the responses generated by the chatbot. A lower temperature will result in more deterministic responses, while a higher temperature will result in more random responses. Experiment with different temperature values to find the right balance between coherence and creativity in the chatbot's responses. 
* Image repository for ilab-client: https://github.com/rafaelvzago/ilab-client[rafaelvzago/ilab-client]
