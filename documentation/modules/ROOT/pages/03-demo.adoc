= See the Solution in Action

== Demonstration

References:

* https://github.com/instructlab[Commands extract from the InstructLab project]
* https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab[Getting started with InstructLab: Generative AI model tuning]
* https://github.com/instructlab/instructlab/blob/main/README.md#-installing-ilab[InstructLab installation guide]
* https://github.com/rafaelvzago/ilab-client[ILAB Frontend chatbot]
* https://developers.redhat.com[Red Hat Developer]

== Concepts and Commands Used in the Demonstration

[NOTE]
====
Please make sure to explain what each Skupper command does the first time you use it, especially for people unfamiliar with Skupper. The following commands should be explained:

- **`skupper init`**: Initializes the Skupper network, setting up the necessary components to enable secure communication between services.
- **`skupper expose`**: Exposes a local service through the Skupper network, allowing it to be accessed from other Skupper-connected sites.
- **`skupper token`**: Generates a connection token that can be used by other sites to link to the Skupper network, ensuring secure communication.
- **`skupper link`**: Establishes a secure link between two Skupper sites using the token created by `skupper token`.
- **`skupper service status`**: Displays the status of services exposed through Skupper, showing what is accessible and how it’s connected within the network.
- **`ilab download`**: Downloads the model to be used by the chatbot.
- **`ilab model serve`**: Starts the server that will be responsible for receiving the user input, sending it to the LLaMA3 model, and sending the response back to the user.
- **`ilab model chat`**: Starts the chatbot, allowing the user to interact with it.

====

== Run the demonstration

=== AI Model Deployment with InstructLab

The first step is to deploy the InstructLab chat model in the InstructLab site. The InstructLab chat model will be responsible for receiving the user input and sending it to the LLaMA3 model. The response from the LLaMA3 model will be sent back to the user. This is based on the article: https://developers.redhat.com/blog/2024/06/12/getting-started-instructlab-generative-ai-model-tuning#model_alignment_and_training_with_instructlab[Getting Started with InstructLab for Generative AI Model Tuning].

[.console-input]
[source,shell script]
----
mkdir instructlab && cd instructlab
python3.11 -m venv venv
source venv/bin/activate
pip install 'instructlab[cuda]' -C cmake.args="-DLLAMA_CUDA=on" -C cmake.args="-DLLAMA_NATIVE=off"
----

[IMPORTANT]
====
This installation method will enable your Nvidia GPU to be used by InstructLab. If you don't have an Nvidia GPU, please check other options in: https://github.com/instructlab/instructlab/blob/main/README.md#-installing-ilab[InstructLab Installation Guide].
====

=== Initialize the InstructLab Configuration

After installing InstructLab, you need to initialize the configuration. This is the step where you create the `config.yaml` file, at `~/instructlab/config.yaml`, with the default configuration. To initialize the configuration, run the following command:

[.console-input]
[source,shell script]
----
ilab config init
----

To enable external access to your model, please modify the `config.yaml` file, located into your `instructlab` directory ie. `~/instructlab/config.yaml`, with the following content:

[source,yaml]
----
chat:
  context: default
  greedy_mode: false
  logs_dir: data/chatlogs
  max_tokens: null
  model: models/granite-7b-lab-Q4_K_M.gguf
  session: null
  vi_mode: false
  visible_overflow: true
general:
  log_level: INFO
generate:
  chunk_word_count: 1000
  model: models/granite-7b-lab-Q4_K_M.gguf
  num_cpus: 10
  num_instructions: 100
  output_dir: generated
  prompt_file: prompt.txt
  seed_file: seed_tasks.json
  taxonomy_base: origin/main
  taxonomy_path: taxonomy
serve:
  gpu_layers: -1
  host_port: 0.0.0.0:8000
  max_ctx_size: 4096
  model_path: models/granite-7b-lab-Q4_K_M.gguf
----

=== Download the model

Now you need to download the model, this is the step where you download the model to be used by the chatbot. The model is the granite-7b-lab-Q4_K_M.gguf model, which is a large language model trained on the Merlinite dataset. To download the model, run the following command:

[.console-input]
[source,shell script]
----
ilab download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

=== Start the server

The last step is to start the server. The server will be responsible for receiving the user input, sending it to the LLaMA3 model, and sending the response back to the user.

[.console-input]
[source,shell script]
----
ilab model serve

# The output should be similar to:
INFO 2024-07-30 18:59:01,199 serve.py:51: serve Using model 'models/merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-07-30 18:59:01,611 server.py:218: server Starting server process, press CTRL+C to shutdown server...
INFO 2024-07-30 18:59:01,612 server.py:219: server After application startup complete see http://0.0.0.0:8000/docs for API.
----

== Public Skupper Deployment

Deploy the public Skupper in Site B. The public Skupper will receive the connection from the private Skupper and create a secure connection between the two sites.

=== Creating the project and deploying the public Skupper:

This is the step where you create the project and deploy the public Skupper. The public Skupper will be responsible for receiving the connection from the private Skupper and creating a secure connection between the two sites. Open a new terminal and run the following commands:


[.console-input]
[source,shell script]
----
export SKUPPER_PLATFORM=kubernetes
oc new-project ollama-pilot
skupper init --enable-console --enable-flow-collector --console-user admin --console-password admin
----

[IMPORTANT]
====
* Run this command in a new terminal and keep it open, because the default platform is `kubernetes` and the private terminal is using `podman`.
====

[NOTE]
====
* `SKUPPER_PLATFORM=kubernetes` is used to set the platform to Kubernetes. This is necessary because the public Skupper will be running on a Kubernetes cluster.
* `oc new-project ollama-pilot` is used to create a new project called `ollama-pilot`.
* `skupper init` is used to initialize the Skupper network, setting up the necessary components to enable secure communication between services.
* The `--enable-console` flag is used to enable the Skupper console, which provides a web interface for managing the Skupper network.
* The `--enable-flow-collector` flag is used to enable the flow collector, which collects and displays information about the traffic flowing through the Skupper network.
* The `--console-user admin` flag is used to set the username for the Skupper console to `admin`.
* The `--console-password admin` flag is used to set the password for the Skupper console to `admin`.
====

=== Creating the token to allow the private Skupper to connect to the public Skupper:

This is the step where you create the token to allow the private Skupper to connect to the public Skupper. At the same terminal, run the following command:

[.console-input]
[source,shell script]
----
skupper token create token.yaml
----

[NOTE]
====
* `skupper token create token.yaml` is used to generate a connection token that can be used by other sites to link to the Skupper network, ensuring secure communication.
* The `token.yaml` file will contain the token to connect the two sites.
====

Now, you'll have a `token.yaml` file with the token to connect the two sites.

== Private Skupper Deployment

The second step is to deploy the private Skupper in Site A. The private Skupper will be responsible for creating a secure connection between the two sites, allowing the Ollama Pilot application to send requests to the LLaMA3 model and receive the response from the merlinite model. Open a new terminal and run the following commands:

=== Install Skupper

To install skupper on site A, with podman as the platform, open a new terminal to handle all the commands related to the private Skupper.

[.console-input]
[source,shell script]
----
export SKUPPER_PLATFORM=podman
skupper init --ingress none
----

[NOTE]
====
* `SKUPPER_PLATFORM=podman` is used to set the platform to podman. This is necessary because the private Skupper will be running on a podman container.
* `skupper init` is used to initialize the Skupper network, setting up the necessary components to enable secure communication between services.
* The `--ingress none` flag is used to disable the automatic creation of an ingress controller. This is necessary because the public Skupper will be responsible for exposing the service to the internet.
====

=== Exposing the InstructLab Chat Model

To bind the local service running the InstructLab chat model to the Skupper service:

[.console-input]
[source,shell script]
----
skupper expose host host.containers.internal --address instructlab --port 8000
----

[NOTE]
====
* `skupper expose` is used to expose a local service through the Skupper network, allowing it to be accessed from other Skupper-connected sites.
* `host.containers.internal` is used to bind the local service to the Skupper service.
* `--address instructlab` is used to specify the address of the service.
* `--port 8000` is used to specify the port of the service.
====

Check the status of the Skupper service:

[.console-input]
[source,shell script]
----
skupper service status

Services exposed through Skupper:
╰─ instructlab:8000 (tcp)
----

[NOTE]
====
* `skupper service status` is used to display the status of services exposed through Skupper, showing what is accessible and how it’s connected within the network.
====

=== Secure Communication Between the Two Sites with Skupper

Now it's time to establish a secure connection between the two sites using the token created by the public Skupper. Using the token created by the public Skupper, run the following command at the terminal where the private Skupper is running:

[.console-input]
[source,shell script]
----
skupper link create token.yaml --name instructlab
----

[NOTE]
====
* `skupper link create token.yaml --name instructlab` is used to establish a secure link between two Skupper sites using the token created by `skupper token`.
====

Check the status of the Skupper link:

[.console-input]
[source,shell script]
----
skupper link status

Links created from this site:

        Link instructlab is connected

Current links from other sites that are connected:

        There are no connected links
----

[NOTE]
====
* `skupper link status` is used to display the status of the links created by the Skupper network, showing which sites are connected and how they are connected.
====

Check the status on the public Skupper terminal:

[.console-input]
[source,shell script]
----
skupper link status

Links created from this site:

       There are no links configured or connected

Current links from other sites that are connected:

       Incoming link from site b8ad86d5-9680-4fea-9c07-ea7ee394e0bd
----

[NOTE]
====
* `skupper link status` is used to display the status of the links created by the Skupper network, showing which sites are connected and how they are connected.
====

=== Chatbot with Protected Data

The last step is to expose the service in the public Skupper and create the Ollama Pilot application.

* Still on the terminal where the **public** Skupper is running, run the following command to expose the service:

[.console-input]
[source,shell script]
----
skupper service create instructlab 8000
----
* Exposing the service to the internet, this command will be executed on the terminal where the **public** Skupper is running and will create a route to expose the service to the internet:

[.console-input]
[source,shell script]
----
oc expose service instructlab
----

[NOTE]
====
* `skupper service create instructlab 8000` is used to create a service in the public Skupper, allowing it to be accessed from the private Skupper.
* `oc expose service instructlab` is used to expose the service to the internet, allowing it to be accessed by the Ollama Pilot application.
====

* Getting the public URL:

This URL will be used to access the chatbot from the Ollama Pilot application.

[.console-input]
[source,shell script]
----
oc get route instructlab
NAME          HOST/PORT                                      PATH          SERVICES                PORT       TERMINATION   WILDCARD
instructlab   instructlab-ollama-pilot.apps.your-cluster-url instructlab                           port8000   None
----

[NOTE]
====
* `oc get route instructlab` is used to get the public URL of the service, which will be used to access the chatbot from the Ollama Pilot application.
====

== Finally, to interact with the chatbot

Before run the chatbot, let's understand the final part of this solution, the Frontend application.

This application will be deployed in a OpenShift cluster, and will be responsible for sending the user input to the InstructLab chat model and displaying the response to the user. The application will be deployed at the same namespace where the public Skupper is running.

=== Deploy ILAB Frontend chatbot

To deploy the ILAB Frontend chatbot, lets use the following yaml deployment file, in this case the file is located at `~/instructlab/ilab-client-deployment.yaml`:

[.source,yaml]
----
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  name: ilab-client
spec:
  replicas: 1
  selector:
    app: ilab-client
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: ilab-client
    spec:
      containers:
      - name: ilab-client-container
        image: quay.io/rzago/ilab-client:latest
        ports:
        - containerPort: 5000
        env:
        - name: ADDRESS
          value: "http://instructlab:8000" # The address of the InstructLab chat model connected to the private Skupper
  triggers:
  - type: ConfigChange
----

Apply the deployment file:

[.console-input]
[source,shell script]
----
oc apply -f ~/instructlab/ilab-client-deployment.yaml
----

[NOTE]
====
* The `ilab-client-deployment.yaml` file is used to deploy the ILAB Frontend chatbot, which will be responsible for sending the user input to the InstructLab chat model and displaying the response to the user.
* The `ADDRESS` environment variable is used to specify the address of the InstructLab chat model connected to the private Skupper.
* The `oc apply -f ~/instructlab/ilab-client-deployment.yaml` command is used to apply the deployment file and deploy the ILAB Frontend chatbot.
====

=== Creating the service of the ILAB Frontend chatbot deployment

Now, let's create the service for the ILAB Frontend chatbot deployment, the file is located at `~/instructlab/ilab-client-service.yaml`:

[.source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: ilab-client-service
spec:
  selector:
    app: ilab-client
  ports:
  - protocol: TCP
    port: 5000
    targetPort: 5000
----

Apply the service file:

[.console-input]
[source,shell script]
----
oc apply -f ~/instructlab/ilab-client-service.yaml
----

[NOTE]
====
* The `ilab-client-service.yaml` file is used to create the service for the ILAB Frontend chatbot deployment.
* The `oc apply -f ~/instructlab/ilab-client-service.yaml` command is used to apply the service file and create the service for the ILAB Frontend chatbot deployment.
====

=== Exposing the service of the ILAB Frontend chatbot deployment

We are almost there, now let's expose the service of the ILAB Frontend chatbot deployment, the file is located at `~/instructlab/ilab-client-route.yaml`:

[.source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: ilab-client-route
spec:
  to:
    kind: Service
    name: ilab-client-service
  port:
    targetPort: 5000
----

Apply the route file:

[.console-input]
[source,shell script]
----
oc apply -f ~/instructlab/ilab-client-route.yaml
----

[NOTE]
====
* The `ilab-client-route.yaml` file is used to expose the service of the ILAB Frontend chatbot deployment.
* The `oc apply -f ~/instructlab/ilab-client-route.yaml` command is used to apply the route file and expose the service of the ILAB Frontend chatbot deployment.
====

=== Accessing the ILAB Frontend chatbot

Finally, to access the ILAB Frontend chatbot, you can use the following command to get the public URL:

[.console-input]
[source,shell script]
----
oc get route ilab-client-route
----

[NOTE]
====
* The `oc get route ilab-client-route` command is used to get the public URL of the ILAB Frontend chatbot, which will be used to access the chatbot from the Ollama Pilot application.
====

== Interacting with the chatbot

To interact with the chatbot, you can access the public URL of the ILAB Frontend chatbot using a web browser. The chatbot will be displayed on the screen, and you can start interacting with it by typing messages in the chat window.


image::chat_bot.png[Chatbot]

=== Considerations

* If your machine has an Nvidia GPU, you can use the InstructLab chat model to generate responses to user input. The InstructLab chat model is a large language model trained on the Merlinite dataset and is capable of generating human-like responses to user input. By following the steps outlined in this demonstration, you can deploy the InstructLab chat model in your environment and interact with it using the ILAB Frontend chatbot. This will allow you to experience the power of generative AI models and see how they can be used to create engaging and interactive applications.
* Adjust the model temperature to control the randomness of the responses generated by the chatbot. A lower temperature will result in more deterministic responses, while a higher temperature will result in more random responses. Experiment with different temperature values to find the right balance between coherence and creativity in the chatbot's responses. 
* Image repository for ilab-client: https://github.com/rafaelvzago/ilab-client[rafaelvzago/ilab-client]
